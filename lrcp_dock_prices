# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
import dataiku
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Read recipe inputs
records_for_pricing = dataiku.Dataset("records_for_pricing")
data = records_for_pricing.get_dataframe()

# Separate features and target
X = data.drop(columns=['Unit Full Price (USD)'])
y = data['Unit Full Price (USD)']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Identify numeric and categorical columns
numeric_features = ['Season']
categorical_features = [col for col in X.columns if col != 'Season']

# Create preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ]), numeric_features),
        ('cat', Pipeline([
            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
            ('onehot', OneHotEncoder(sparse=False, handle_unknown='ignore'))
        ]), categorical_features)
    ])

# Create a pipeline with RandomForestRegressor
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))
])

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE

# Fit the pipeline on the training data
pipeline.fit(X_train, y_train)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE


# Make predictions on the test set
y_pred = pipeline.predict(X_test)

# Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

# Create a dataframe with the evaluation metrics
evaluation_df = pd.DataFrame({
    'Metric': ['Mean Squared Error', 'Root Mean Squared Error', 'R-squared', 'Mean Absolute Error'],
    'Value': [mse, rmse, r2, mae]
})

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Output the evaluation metrics to the new dataset
price_model_evaluation = dataiku.Dataset("price_model_evaluation")
price_model_evaluation.write_with_schema(evaluation_df)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
import itertools

def generate_combinations(data, max_combinations=100000):
    # Ensure we're using all columns from the original dataset
    all_columns = data.columns.tolist()
    
    location_channel = ['location_lrcp_group', 'SubChannel']
    pc5_related = ['Consumer', 'Category', 'SubCategory 1', 'Class', 'SubClass', 'pc5_calc']
    
    # Generate combinations for location and channel
    location_channel_combinations = data[location_channel].drop_duplicates()
    
    # Get unique seasons
    seasons = data['Season'].unique()
    
    # Sample unique pc5 combinations
    pc5_combinations = data[pc5_related].drop_duplicates()
    
    # Calculate the number of combinations
    total_combinations = len(location_channel_combinations) * len(seasons) * len(pc5_combinations)
    
    if total_combinations > max_combinations:
        # If total combinations exceed max_combinations, sample pc5_combinations
        sample_size = max_combinations // (len(location_channel_combinations) * len(seasons))
        pc5_combinations = pc5_combinations.sample(n=sample_size, random_state=42)
    
    # Generate all combinations using itertools.product
    all_combinations = list(itertools.product(
        location_channel_combinations.itertuples(index=False),
        seasons,
        pc5_combinations.itertuples(index=False)
    ))
    
    # Convert to DataFrame
    combinations = pd.DataFrame([
        (*loc_channel, season, *pc5)
        for loc_channel, season, pc5 in all_combinations
    ], columns=location_channel + ['Season'] + pc5_related)
    
    # Ensure all columns from the original dataset are present
    for col in all_columns:
        if col not in combinations.columns:
            combinations[col] = data[col].iloc[0]  # Use the first value as a placeholder
    
    # Reorder columns to match the original dataset
    combinations = combinations[all_columns]
    
    # If the number of combinations is still too high, sample randomly
    if len(combinations) > max_combinations:
        print(f"Warning: Number of combinations ({len(combinations)}) exceeds the maximum allowed ({max_combinations}). Sampling randomly.")
        combinations = combinations.sample(n=max_combinations, random_state=42)
    
    return combinations

# Generate combinations
all_combinations = generate_combinations(X)

# Make predictions for all combinations
predictions = pipeline.predict(all_combinations)

# Add predictions to the combinations dataframe
all_combinations['Predicted_Price'] = predictions

# Sort the results
output_table = all_combinations.sort_values(all_combinations.columns.tolist())

# Write to output dataset
price_model2 = dataiku.Dataset("price_model2")
price_model2.write_with_schema(output_table)

print("Model evaluation metrics have been written to 'price_model_evaluation' dataset.")
print("Predicted prices for combinations have been written to 'price_model' dataset.")
